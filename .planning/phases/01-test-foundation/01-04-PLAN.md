---
phase: 01-test-foundation
plan: 04
type: execute
wave: 2
depends_on: ["01-01", "01-02"]
files_modified:
  - docs/audits/test-quality-audit.md
autonomous: true

must_haves:
  truths:
    - "All time.Sleep() violations in test files are documented with file, line, and suggested fix"
    - "Global state dependencies in tests are identified"
    - "Non-deterministic data generation patterns are catalogued"
    - "Coverage gaps in relayer/ and cache/ packages are noted"
    - "Flaky tests (if any) are documented with skip pattern"
    - "100-run stability results are documented with timestamp and pass/fail count"
  artifacts:
    - path: "docs/audits/test-quality-audit.md"
      provides: "Comprehensive test quality audit"
      contains: "time.Sleep"
    - path: "docs/audits/test-quality-audit.md"
      provides: "Flaky test documentation"
      contains: "Flaky Test"
    - path: "docs/audits/test-quality-audit.md"
      provides: "100-run stability results"
      contains: "100-Run Results"
  key_links:
    - from: "docs/audits/test-quality-audit.md"
      to: "Phase 3"
      via: "Audit informs test cleanup work"
      pattern: "TODO.*phase.?3"
---

<objective>
Create comprehensive test quality audit and run stability validation

Purpose: Document all test quality violations (time.Sleep, global state, non-determinism) to inform Phase 3 cleanup work, and validate current test stability by running the 100-run stability script. This completes QUAL-02 (flaky test audit) and QUAL-03 (deterministic data identification).

Output: `docs/audits/test-quality-audit.md` with all violations documented. Any flaky tests found will be skipped with TODO comments.
</objective>

<execution_context>
@/home/overlordyorch/.claude/get-shit-done/workflows/execute-plan.md
@/home/overlordyorch/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/01-test-foundation/01-CONTEXT.md
@.planning/phases/01-test-foundation/01-RESEARCH.md
@CLAUDE.md
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create test quality audit document</name>
  <files>docs/audits/test-quality-audit.md</files>
  <action>
1. Create the `docs/audits/` directory if it doesn't exist

2. Run grep to find all time.Sleep violations in test files:
   `grep -rn "time.Sleep" --include="*_test.go" . | grep -v vendor/`

3. Run grep to find potential global state (package-level var with mutation):
   `grep -rn "^var " --include="*_test.go" . | grep -v vendor/`

4. Run grep to find random data generation without seeds:
   `grep -rn "rand\." --include="*_test.go" . | grep -v vendor/`

5. Check coverage for relayer/ and cache/ packages:
   `go test -tags test -cover ./relayer/... ./cache/... 2>&1`

6. Create `docs/audits/test-quality-audit.md` with the following structure:

```markdown
# Test Quality Audit

**Date:** YYYY-MM-DD
**Phase:** 01-test-foundation
**Purpose:** Document test quality violations for Phase 3 cleanup
**Status:** Snapshot - delete when all violations fixed

## Summary

| Category | Count | Priority |
|----------|-------|----------|
| time.Sleep violations | XX | HIGH (causes flaky tests) |
| Global state | XX | MEDIUM (cross-test contamination) |
| Non-deterministic data | XX | MEDIUM (reproducibility) |
| Missing race tests | XX | HIGH (per Rule #1) |

## time.Sleep Violations

Per CLAUDE.md Rule #1: "No flaky tests, no race conditions"
time.Sleep in tests is an anti-pattern that causes flaky behavior.

| File | Line | Context | Suggested Fix |
|------|------|---------|---------------|
| path/to/file_test.go | XX | Description | Use channels/Eventually/synctest |
...

## Global State Dependencies

Package-level mutable state can cause cross-test contamination.

| File | Line | Variable | Issue |
|------|------|----------|-------|
...

## Non-Deterministic Data Generation

Tests using random data without seeds are not reproducible.

| File | Line | Pattern | Suggested Fix |
|------|------|---------|---------------|
...

## Coverage Gaps

| Package | Coverage | Status |
|---------|----------|--------|
| miner/ | XX% | Comprehensive tests exist |
| relayer/ | XX% | No unit tests (covered by scripts/) |
| cache/ | XX% | Tests exist |

## Flaky Tests Identified

Tests that failed during 100-run stability check.

| Test | Package | Failure Rate | Status |
|------|---------|--------------|--------|
| (none or list) | | | Skipped with TODO |

## 100-Run Results

**Executed:** YYYY-MM-DD HH:MM:SS UTC
**Result:** XX/100 runs passed
**Failing tests:** (list or "None")

---

*Audit completed: YYYY-MM-DD*
*Delete this file when all violations are fixed in Phase 3*
```

Fill in actual data from grep results. Research indicated 64+ time.Sleep violations - audit should confirm and categorize all of them.
  </action>
  <verify>
1. Directory `docs/audits/` exists
2. File `docs/audits/test-quality-audit.md` exists
3. File contains sections for: time.Sleep, Global State, Non-Deterministic Data, Coverage Gaps, Flaky Tests, 100-Run Results
4. File contains actual grep results (not placeholders)
  </verify>
  <done>
Audit document created with all test quality violations catalogued. time.Sleep count matches or exceeds the 64+ identified in research.
  </done>
</task>

<task type="auto">
  <name>Task 2: Run stability test and document flaky tests</name>
  <files>docs/audits/test-quality-audit.md</files>
  <action>
1. Run the stability script with a reduced count first to validate it works:
   `RUNS=10 ./scripts/test-stability.sh`

2. If tests pass 10/10, run the full 100-run stability test:
   `./scripts/test-stability.sh`
   Note: This may take 30-60 minutes with race detection enabled.

3. If any tests fail during the 100-run check:
   a. Identify the failing test from the output
   b. Add `t.Skip("TODO(phase3): fix flaky test - <brief reason>")` at the start of the test function
   c. Document the test in the "Flaky Tests Identified" section of the audit document
   d. Re-run stability test to ensure the skip works

4. Update the audit document's "100-Run Results" section with REQUIRED fields:
   - **Executed:** Timestamp of when the 100-run test was executed (YYYY-MM-DD HH:MM:SS UTC)
   - **Result:** Total pass count (e.g., "100/100 runs passed" or "98/100 runs passed")
   - **Failing tests:** List of any failing tests with failure counts, or "None" if all passed

5. Update the "Flaky Tests Identified" section:
   - If no flaky tests found: Add row with "(none detected)" in Test column
   - If flaky tests found: List each one with package, failure rate (e.g., "2/100"), and skip status

6. Commit message for any test skips should reference this plan:
   `test(01-04): skip flaky test for Phase 3 fix - <test name>`

Per 01-CONTEXT.md: "Policy for existing flaky tests: Skip with t.Skip() and TODO comment, fix in Phase 3"
  </action>
  <verify>
1. `RUNS=10 ./scripts/test-stability.sh` completes successfully (exit code 0)
2. Full 100-run stability test was executed
3. Audit document contains "100-Run Results" subsection with ALL of:
   - "Executed:" line with timestamp (e.g., "Executed: 2026-02-02 15:30:00 UTC")
   - "Result:" line with pass/fail count (e.g., "Result: 100/100 runs passed")
   - "Failing tests:" line with list or "None"
4. Any flaky tests are either:
   - Fixed (if trivial), OR
   - Skipped with `t.Skip("TODO(phase3): ...")` and documented in audit
5. Audit document's "Flaky Tests Identified" section is populated (even if with "(none detected)")
  </verify>
  <done>
Stability test completed. Audit document contains "100-Run Results" subsection with timestamp (Executed:), pass/fail count (Result:), and failing test list (Failing tests:). All flaky tests either fixed or skipped with TODO comments.
  </done>
</task>

</tasks>

<verification>
1. `docs/audits/test-quality-audit.md` exists with complete audit data
2. All time.Sleep violations are documented with file paths and line numbers
3. Coverage gaps for relayer/ and cache/ are noted
4. Audit document contains "100-Run Results" subsection with:
   - Timestamp (Executed: YYYY-MM-DD HH:MM:SS UTC)
   - Pass/fail count (Result: XX/100 runs passed)
   - Failing test list (Failing tests: list or None)
5. Any flaky tests are skipped with `t.Skip("TODO(phase3): ...")` pattern
6. Grep for `t.Skip.*TODO.*phase` shows any skipped tests (or returns empty if none)
</verification>

<success_criteria>
- Audit document completed with actual violation counts (not placeholders)
- time.Sleep violations documented (research indicated 64+)
- 100-run stability test completed
- Audit document "100-Run Results" section contains: timestamp, pass/fail count, failing test list
- All existing tests pass 100/100 runs (flaky tests skipped if necessary)
- Flaky tests documented in audit with skip status
</success_criteria>

<output>
After completion, create `.planning/phases/01-test-foundation/01-04-SUMMARY.md`
</output>
