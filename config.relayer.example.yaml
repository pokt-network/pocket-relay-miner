# Pocket RelayMiner Relayer Configuration Example
# This file uses LOCALNET defaults - ready for local development with Tilt.
# For PRODUCTION, see comments marked [PRODUCTION] for recommended values.
#
# The relayer is a stateless HTTP/WebSocket/gRPC/Streaming proxy

# HTTP server listen address
listen_addr: "0.0.0.0:8080"

# Redis connection (required for publishing relays to miners)
redis:
  # Single Redis instance
  url: "redis://localhost:6379"

  # OR Redis Cluster (comment out url above, uncomment below)
  # url: "redis://redis-node1:6379,redis-node2:6379,redis-node3:6379"

  # Connection pool settings (2x go-redis defaults for production)
  # Omit these to use defaults (auto-calculated based on CPU cores)
  # pool_size: 320                   # Max connections (default: 20 Ã— runtime.GOMAXPROCS)
  # min_idle_conns: 80               # Warm idle connections (default: pool_size / 4)
  # pool_timeout_seconds: 4          # Wait time for connection from pool
  # conn_max_idle_time_seconds: 300  # Close idle connections after 5 minutes

  # Namespace configuration (optional - defaults match historical "ha:" prefix structure)
  # All Redis key prefixes are derived from this config.
  # Must match miner configuration for proper operation.
  # namespace:
  #   base_prefix: "ha"                # Root prefix for all keys (default: "ha")
  #   cache_prefix: "cache"            # Cache data: ha:cache:* (default: "cache")
  #   events_prefix: "events"          # Pub/sub channels: ha:events:* (default: "events")
  #   streams_prefix: "relays"         # Relay streams: ha:relays:* (default: "relays")
  #   miner_prefix: "miner"            # Miner state: ha:miner:* (default: "miner")
  #   supplier_prefix: "supplier"      # Supplier data: ha:supplier:* (default: "supplier")
  #   meter_prefix: "meter"            # Metering data: ha:meter:* (default: "meter")
  #   params_prefix: "params"          # Cached params: ha:params:* (default: "params")
  #   consumer_group_prefix: "miners"  # Consumer group: ha-miners (default: "miners")

# Pocket blockchain connection
pocket_node:
  # RPC endpoint for block subscriptions and queries (localnet defaults)
  query_node_rpc_url: "http://localhost:26657"
  # [PRODUCTION] query_node_rpc_url: "https://rpc.testnet.pokt.network"

  # gRPC endpoint for queries (sessions, applications, services)
  query_node_grpc_url: "localhost:9090"
  # [PRODUCTION] query_node_grpc_url: "grpc.testnet.pokt.network:443"

  # gRPC settings
  grpc_insecure: true  # Disable TLS for gRPC (localnet)
  # [PRODUCTION] grpc_insecure: false

  # NOTE: Relayers always use Redis pub/sub for block events (synchronized with miner).
  # RPC/gRPC endpoints are used for health checks and fallback queries only.

# Supplier signing keys (required for signing relay responses)
keys:
  # Option 1: Load from YAML file (simple, recommended for testing)
  keys_file: "/keys/supplier-keys.yaml"

  # Option 2: Load from directory (one file per key)
  # keys_dir: "/keys/supplier-keys"

  # Option 3: Use Cosmos SDK keyring (recommended for production)
  # keyring:
  #   backend: "file"  # file, os, test, memory
  #   dir: "/path/to/keyring"
  #   app_name: "pocket"
  #   key_names:  # Optional: specific keys to load
  #     - "supplier1"
  #     - "supplier2"

# Service configurations (map of service_id -> config)
# Genesis defines 4 services: develop-http, develop-websocket, develop-stream, develop-grpc
# Each has 15 suppliers staked for it.
services:
  # HTTP/JSON-RPC service (localnet)
  develop-http:
    validation_mode: eager
    timeout_profile: fast
    max_body_size_bytes: 10485760  # 10MB
    default_backend: jsonrpc  # Use jsonrpc for proper protocol labeling (nginx backend still available)
    backends:
      # Full-featured demo backend - dynamic responses
      # Use for: Feature testing, WebSocket, gRPC, streaming
      jsonrpc:
        url: "http://backend:8545"
        # [PRODUCTION] url: "http://eth-node:8545"
        # Optional headers/auth:
        # headers:
        #   X-Custom-Header: "value"
        # authentication:
        #   bearer_token: "your-token"

  # WebSocket service (localnet)
  develop-websocket:
    validation_mode: eager
    timeout_profile: fast
    max_body_size_bytes: 10485760
    default_backend: websocket
    backends:
      websocket:
        url: "ws://backend:8545/ws"
        # [PRODUCTION] url: "ws://eth-node:8545/ws"

  # Streaming/SSE service (localnet)
  develop-stream:
    validation_mode: eager
    timeout_profile: streaming  # 10 min timeout for streaming responses
    max_body_size_bytes: 10485760
    default_backend: rest
    backends:
      rest:
        url: "http://backend:8545/stream/sse"
        # [PRODUCTION] url: "http://eth-node:8545/stream/sse"

  # gRPC service (localnet)
  develop-grpc:
    validation_mode: eager
    timeout_profile: fast
    max_body_size_bytes: 10485760
    default_backend: grpc
    backends:
      grpc:
        url: "backend:50051"
        # [PRODUCTION] url: "eth-node:50051"

  # Example: text-generation service with streaming profile
  # This demonstrates using the "streaming" timeout profile for long-running LLM requests
  # The profile defines request_timeout_seconds (600s) and response_header_timeout (0 for streaming)
  # text-generation:
  #   validation_mode: eager
  #   timeout_profile: streaming  # 10 min request timeout, no header timeout for streaming
  #   backends:
  #     jsonrpc:
  #       url: "https://llm-backend.example.com"
  #       authentication:
  #         bearer_token: "your-api-key"

  # Example: eth-mainnet service
  # eth-mainnet:
  #   validation_mode: optimistic
  #   default_backend: jsonrpc
  #   backends:
  #     jsonrpc:
  #       url: "https://mainnet.infura.io/v3/YOUR_PROJECT_ID"
  #       authentication:
  #         bearer_token: "YOUR_API_KEY"
  #     websocket:
  #       url: "wss://mainnet.infura.io/ws/v3/YOUR_PROJECT_ID"

  # Example: Solana service
  # solana-mainnet:
  #   validation_mode: eager
  #   request_timeout_seconds: 60  # Solana can be slower
  #   default_backend: jsonrpc
  #   backends:
  #     jsonrpc:
  #       url: "https://api.mainnet-beta.solana.com"

# Default validation mode (used when not specified per-service)
# Options: "eager" | "optimistic"
default_validation_mode: optimistic

# Default request timeout in seconds (used when not specified per-service)
default_request_timeout_seconds: 30

# Default max body size in bytes (used when not specified per-service)
default_max_body_size_bytes: 10485760  # 10MB

# Metrics server (Prometheus)
metrics:
  enabled: true
  addr: "0.0.0.0:9090"

# Pprof profiling (for debugging, disable in production)
pprof:
  enabled: true  # Enabled by default for localnet debugging
  addr: "0.0.0.0:6060"
  # [PRODUCTION] enabled: false
  # [PRODUCTION] addr: "localhost:6060"

# Health check endpoint
health_check:
  enabled: true
  addr: "0.0.0.0:8081"

# Relay meter (rate limiting based on app stakes)
relay_meter:
  # Enable metering and rate limiting
  enabled: true

  # Redis key prefix for metering data
  redis_key_prefix: "ha"

  # Fail behavior when Redis is unavailable
  # - "open": Allow relays when Redis down (prioritize availability)
  # - "closed": Reject relays when Redis down (prioritize safety)
  fail_behavior: "open"

  # Cache TTL for all Redis data (streams, params, app stakes, meters)
  # Redis TTL handles automatic expiration - no cleanup goroutines needed.
  # Default: 2h (covers ~15 session lifecycles at 30s blocks)
  cache_ttl: 2h

# HTTP transport settings (connection pooling and timeouts for backend requests)
# These settings optimize connection reuse and prevent resource exhaustion.
# Defaults are tuned for 1000+ RPS - override based on your backend characteristics.
http_transport:
  # Connection pool limits (5x increased for high throughput)
  max_idle_conns: 500  # Total idle connections across all backends (default: 500)
  max_idle_conns_per_host: 100  # Idle connections per backend (default: 100)
  max_conns_per_host: 500  # Total connections per backend to prevent port exhaustion (default: 500, 0 = unlimited)

  # Connection timeout settings
  idle_conn_timeout_seconds: 90  # How long to keep idle connections alive (default: 90)
  dial_timeout_seconds: 5  # Timeout for establishing new connection (default: 5)
  tls_handshake_timeout_seconds: 10  # Timeout for TLS handshake (default: 10)
  response_header_timeout_seconds: 30  # Timeout waiting for response headers (default: 30)
  expect_continue_timeout_seconds: 1  # Timeout for 100-continue response (default: 1)
  tcp_keep_alive_seconds: 30  # TCP keepalive period for active connections (default: 30, 0 = disabled)

  # Content handling
  disable_compression: true  # Don't modify content encoding (required for relay protocol)

# ==============================================================================
# HTTP Timeout Profiles (Advanced)
# ==============================================================================
# Define timeout profiles for different service types.
# Each profile defines request_timeout_seconds and HTTP client timeouts.
# Services select a profile via the timeout_profile field.
# Auto-populated with "fast" and "streaming" defaults if not specified.
timeout_profiles:
  # Fast profile for standard RPC services (default for all services)
  fast:
    request_timeout_seconds: 30  # Overall request timeout (30s default)
    response_header_timeout_seconds: 30  # Timeout waiting for response headers
    dial_timeout_seconds: 5  # Timeout for establishing connection
    tls_handshake_timeout_seconds: 10  # Timeout for TLS handshake

  # Streaming profile for long-running services (LLMs, AI models)
  streaming:
    request_timeout_seconds: 600  # 10 minute timeout for LLM completions
    response_header_timeout_seconds: 0  # No header timeout for streaming responses
    dial_timeout_seconds: 10  # Allow more time for connection
    tls_handshake_timeout_seconds: 15  # Allow more time for TLS

# Cache warmup (optional - preload application data at startup)
cache_warmup:
  # Enable cache warmup (speeds up first requests)
  enabled: true

  # Known applications to pre-load (if you know your app addresses)
  # known_applications:
  #   - "pokt1mrqt5f7qh8uxs27cjm9t7v9e74a9vvdnq5jva4"
  #   - "pokt1abc..."

  # Persist discovered apps to Redis for faster warmup on restart
  persist_discovered_apps: true

  # Concurrency for warmup operations (higher = faster but more load)
  warmup_concurrency: 10

  # Timeout per application warmup
  warmup_timeout_seconds: 5

# Logging configuration
logging:
  level: "debug"  # trace, debug, info, warn, error (localnet uses debug)
  format: "json"  # json, text (json required for Loki log level detection)
  output: "stdout"  # stdout, stderr, or file path
  # [PRODUCTION] level: "info"
